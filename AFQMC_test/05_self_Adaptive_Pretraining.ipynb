{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1fef4a68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fef4a68",
        "outputId": "20eae208-afaa-48d5-a928-afcd544a3ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  9 12:24:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dc9e15cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc9e15cb",
        "outputId": "f4e62155-cd31-448f-a721-a32e7b0abfc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eb14c151",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb14c151",
        "outputId": "7c7a7186-cfd4-4db2-f66c-49bd2630eb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2d91b1f0fed4d6659ad1da2319a8737c1958aa70eace199c57ffc982fc491f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "01ecf1ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ecf1ca",
        "outputId": "a2d652d1-5e69-4047-f4f0-74b28abf2cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torch==1.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "365865c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "365865c6",
        "outputId": "880e91d8-f3c4-4fcb-f523-89ab6888f763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.7.0\n",
            "  Downloading torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.21.6)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "Successfully installed torchvision-0.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torchvision==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5924d70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "5924d70e",
        "outputId": "43ca64a2-8359-4beb-cce1-3b4f0248d985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.17.0\n",
            "  Downloading numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 1.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.17.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.17.0 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.17.0 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.17.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install numpy==1.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cccccd71",
      "metadata": {
        "id": "cccccd71"
      },
      "source": [
        "## logging\n",
        "Transformers 有一个集中的日志记录系统，因此您可以轻松设置库的详细程度。\n",
        "目前该库的默认详细程度是WARNING.\n",
        "* transformers.logging.CRITICALor transformers.logging.FATAL(int value, 50)：只报告最严重的错误。\n",
        "* transformers.logging.ERROR(int value, 40)：只报告错误。\n",
        "* transformers.logging.WARNINGor transformers.logging.WARN(int value, 30)：只报告错误和警告。这是库使用的默认级别。\n",
        "* transformers.logging.INFO(int value, 20)：报告错误、警告和基本信息。\n",
        "* transformers.logging.DEBUG(int value, 10)：上报所有信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef63b10d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef63b10d",
        "outputId": "1cade321-f734-490f-a552-c55c696267e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from transformers import logging\n",
        "from transformers import BertTokenizer\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  return seed\n",
        "\n",
        "logging.set_verbosity_info()\n",
        "seed_everything(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8099e3d9",
      "metadata": {
        "id": "8099e3d9"
      },
      "outputs": [],
      "source": [
        "corpus_path = '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM'\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model'\n",
        "output_dir = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e7183f11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7183f11",
        "outputId": "88cbfa5f-eb6f-449e-8b99-08f974deb97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model name '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/special_tokens_map.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/tokenizer_config.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/vocab.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "796b487c",
      "metadata": {
        "id": "796b487c"
      },
      "outputs": [],
      "source": [
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d47951b0",
      "metadata": {
        "id": "d47951b0"
      },
      "outputs": [],
      "source": [
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(os.path.join(corpus_path, 'train.json'), data_type = 'train')\n",
        "  dev_df = parse_data(os.path.join(corpus_path, 'dev.json'), data_type = 'dev')\n",
        "  test_df = parse_data(os.path.join(corpus_path, 'test.json'), data_type = 'test')\n",
        "\n",
        "  train_df.append(dev_df)\n",
        "  train_df.append(test_df)\n",
        "  inputs = defaultdict(list)\n",
        "\n",
        "\n",
        "  for i, row in tqdm(train_df.iterrows(), desc= f'Preprocessing train data', total = len(train_df)):\n",
        "      inputs_dict = tokenizer.encode_plus(row[0] + row[1], add_special_tokens = True,\n",
        "                                          return_token_type_ids = True, return_attention_mask = True)\n",
        "      inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "      inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "      inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "    \n",
        "    \n",
        "    \n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "afc2b796",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afc2b796",
        "outputId": "e6e69c41-0c47-4292-9c31-4457dde5d849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 241939.10it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 242451.16it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 252860.65it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:27<00:00, 1267.96it/s]\n"
          ]
        }
      ],
      "source": [
        "data = read_data(corpus_path, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  # 返回一个example\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "uIF0E30pjtDO"
      },
      "id": "uIF0E30pjtDO",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AFQMCDataset(data)"
      ],
      "metadata": {
        "id": "z4jLz1EUlxpb"
      },
      "id": "z4jLz1EUlxpb",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM\n",
        "from transformers import LineByLineTextDataset, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "def train(self):\n",
        "    # model_path是预训练模型的位置\n",
        "    # 分词 BertTokenizerFast\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_path, max_len=512)\n",
        "    # 定义一个模型 BertForMaskedLM\n",
        "    model = BertForMaskedLM.from_pretrained(model_path)\n",
        "    # daseset LineByLineTextDataset\n",
        "    dataset = LineByLineTextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=\"语料文件.txt\",\n",
        "        block_size=128,\n",
        "    )\n",
        "    # collator DataCollatorForLanguageModeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=0.10\n",
        "    )\n",
        "    # TrainingArguments 训练参数，使用的接口类\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"xxxx\",\n",
        "        ...各种参数，参见文档或者源码\n",
        "    )\n",
        "    \n",
        "    # 模型训练用Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"保存位置\")\n",
        "```"
      ],
      "metadata": {
        "id": "2SYAtGRXmEiT"
      },
      "id": "2SYAtGRXmEiT"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "model = BertForMaskedLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8PGodkZnEEy",
        "outputId": "39e8215e-cf8d-43b3-f4d5-71bc16b62492"
      },
      "id": "G8PGodkZnEEy",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/dataset/BERT_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOTE1"
      ],
      "metadata": {
        "id": "ylvsMkGrf6qL"
      },
      "id": "ylvsMkGrf6qL"
    },
    {
      "cell_type": "code",
      "source": [
        "# N -Gram N: 1,2,3\n",
        "ngrams = np.arange(1, 4, dtype = np.int64)\n",
        "print('ngrams:',ngrams)\n",
        "pvals = 1. / np.arange(1, 4)\n",
        "print('pvals(1):',pvals) \n",
        "p = pvals.sum(keepdims = True)\n",
        "print('psum:',p)\n",
        "pvals /= pvals.sum(keepdims = True)\n",
        "print('pvals(2):',pvals)\n",
        "pvals = pvals[::-1]\n",
        "print('pvals(3):',pvals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbJmI-7vnS0y",
        "outputId": "f5849f74-e46f-46c6-c75f-075932246955"
      },
      "id": "fbJmI-7vnS0y",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrams: [1 2 3]\n",
            "pvals(1): [1.         0.5        0.33333333]\n",
            "psum: [1.83333333]\n",
            "pvals(2): [0.54545455 0.27272727 0.18181818]\n",
            "pvals(3): [0.18181818 0.27272727 0.54545455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NOTE2"
      ],
      "metadata": {
        "id": "ymLTHi-wgAJa"
      },
      "id": "ymLTHi-wgAJa"
    },
    {
      "cell_type": "code",
      "source": [
        "# 非special token的位置\n",
        "cand_indexes = [[1],[2],[3],[4],[5],[7],[8],[9]]\n",
        "# ngrams = [1,2,3]\n",
        "ngram_indexes = []\n",
        "for idx in range(len(cand_indexes)):\n",
        "  ngram_index = []\n",
        "  for n in ngrams:\n",
        "    # 生成1/2/3-gram\n",
        "    ngram_index.append(cand_indexes[idx:idx + n])\n",
        "    print('ngram_index:',ngram_index)\n",
        "  ngram_indexes.append(ngram_index)\n",
        "\n",
        "print('ngram_indexes:',ngram_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIR98Bh3gCKa",
        "outputId": "2ee30a87-6121-421f-964f-4ec7462b3707"
      },
      "id": "LIR98Bh3gCKa",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngram_index: [[[1]]]\n",
            "ngram_index: [[[1]], [[1], [2]]]\n",
            "ngram_index: [[[1]], [[1], [2]], [[1], [2], [3]]]\n",
            "ngram_index: [[[2]]]\n",
            "ngram_index: [[[2]], [[2], [3]]]\n",
            "ngram_index: [[[2]], [[2], [3]], [[2], [3], [4]]]\n",
            "ngram_index: [[[3]]]\n",
            "ngram_index: [[[3]], [[3], [4]]]\n",
            "ngram_index: [[[3]], [[3], [4]], [[3], [4], [5]]]\n",
            "ngram_index: [[[4]]]\n",
            "ngram_index: [[[4]], [[4], [5]]]\n",
            "ngram_index: [[[4]], [[4], [5]], [[4], [5], [7]]]\n",
            "ngram_index: [[[5]]]\n",
            "ngram_index: [[[5]], [[5], [7]]]\n",
            "ngram_index: [[[5]], [[5], [7]], [[5], [7], [8]]]\n",
            "ngram_index: [[[7]]]\n",
            "ngram_index: [[[7]], [[7], [8]]]\n",
            "ngram_index: [[[7]], [[7], [8]], [[7], [8], [9]]]\n",
            "ngram_index: [[[8]]]\n",
            "ngram_index: [[[8]], [[8], [9]]]\n",
            "ngram_index: [[[8]], [[8], [9]], [[8], [9]]]\n",
            "ngram_index: [[[9]]]\n",
            "ngram_index: [[[9]], [[9]]]\n",
            "ngram_index: [[[9]], [[9]], [[9]]]\n",
            "ngram_indexes: [[[[1]], [[1], [2]], [[1], [2], [3]]], [[[2]], [[2], [3]], [[2], [3], [4]]], [[[3]], [[3], [4]], [[3], [4], [5]]], [[[4]], [[4], [5]], [[4], [5], [7]]], [[[5]], [[5], [7]], [[5], [7], [8]]], [[[7]], [[7], [8]], [[7], [8], [9]]], [[[8]], [[8], [9]], [[8], [9]]], [[[9]], [[9]], [[9]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for cand_index_set in ngram_indexes:\n",
        "  # n 是n-gram的n\n",
        "  n = np.random.choice(ngrams[:len(cand_index_set)],\n",
        "                 p = pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims = True))\n",
        "  print('n:',n)\n",
        "  print('cand_index_set:',cand_index_set[n - 1])\n",
        "  # 合并列表\n",
        "  index_set = sum(cand_index_set[n - 1], [])\n",
        "  print('index_set:',index_set)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct2SEEE5hHjB",
        "outputId": "f6af1fa3-c211-4ea2-a5d3-b953a5dca011"
      },
      "id": "ct2SEEE5hHjB",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n: 1\n",
            "cand_index_set: [[1]]\n",
            "index_set: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### typing List，Tuple\n",
        "List、列表，是 list 的泛型，基本等同于 list，其后紧跟一个方括号，里面代表了构成这个列表的元素类型，如由数字构成的列表可以声明为：\n",
        "```\n",
        "var: List[int or float] = [2, 3.5]\n",
        "```\n",
        "嵌套声明也是可以的\n",
        "\n",
        "Tuple、元组，是 tuple 的泛型，其后紧跟一个方括号，方括号中按照顺序声明了构成本元组的元素类型，如 Tuple[X, Y] 代表了构成元组的第一个元素是 X 类型，第二个元素是 Y 类型。 比如想声明一个元组，分别代表姓名、年龄、身高，三个数据类型分别为 str、int、float，那么可以这么声明：\n",
        "\n",
        "```\n",
        "person: Tuple[str, int, float] = ('Mike', 22, 1.75)\n",
        "```"
      ],
      "metadata": {
        "id": "V1ox3eh9owuM"
      },
      "id": "V1ox3eh9owuM"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "class Collator:\n",
        "  def __init__(self, max_seq_len: int, tokenizer: BertTokenizer, mlm_probability = 0.15):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "    self.mlm_probability = mlm_probability\n",
        "    self.special_token_ids = {tokenizer.cls_token_id, tokenizer.sep_token_id}\n",
        "  \n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len),dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    \n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype = torch.long)\n",
        "        token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "        attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "      else:\n",
        "        # input_ids 最后一位放上一个特殊的token\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len - 1] + [self.tokenizer.sep_token_id], dtype = torch.long)\n",
        "        # token_type_ids 和 attention_mask 不需要加上特殊token\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype = torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype = torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "  def _ngram_mask(self, input_ids, max_seq_len, seed):\n",
        "    cand_indexes = []\n",
        "    # 哪些不是 special_token, 并记录不是 special_token 的位置，special_token不需要被预测\n",
        "    for (i, id_) in enumerate(input_ids):\n",
        "      if id_ in self.special_token_ids:\n",
        "        continue\n",
        "      cand_indexes.append([i])\n",
        "      # cand_indexes [[1],[2]]\n",
        "    \n",
        "    # 要 mask 的数量， BERT中被 MASK 的比例15% \n",
        "    num_to_predict = max(1, int(round(len(input_ids) * self.mlm_probability)))\n",
        "    # ------------------------------Note1------------------------------- #\n",
        "    ngrams = np.arange(1, 4, dtype = np.int64)\n",
        "    pvals = 1. / np.arange(1, 4)\n",
        "\n",
        "    pvals /= pvals.sum(keepdims = True)\n",
        "\n",
        "    pvals = pvals[::-1]\n",
        "    # ------------------------------Note1------------------------------- #\n",
        "\n",
        "    # ------------------------------Note2------------------------------- #\n",
        "    # 1，2，3gram\n",
        "    ngram_indexes = []\n",
        "    for idx in range(len(cand_indexes)):\n",
        "      ngram_index = []\n",
        "      for n in ngrams:\n",
        "        ngram_index.append(cand_indexes[idx:idx + n])\n",
        "      ngram_indexes.append(ngram_index) \n",
        "    # ------------------------------Note2------------------------------- #\n",
        "    \n",
        "    np.random.shuffle(ngram_indexes)\n",
        "    # covered_indexes mask 掉 n-gram的集合\n",
        "    covered_indexes = set()\n",
        "\n",
        "    for cand_index_set in ngram_indexes:\n",
        "      # 要 mask 的数量 >= 规定的mask数量\n",
        "      if len(covered_indexes) >= num_to_predict:\n",
        "        break\n",
        "      if not cand_index_set:\n",
        "        continue\n",
        "\n",
        "      # ------------------------------Note3------------------------------- #\n",
        "      # 按照概率挑选 n-gram n=1,2,3\n",
        "      n = np.random.choice(ngrams[:len(cand_index_set)], p = pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims = True))\n",
        "\n",
        "      # 举例 n=3, 代表的是3-gram, 将[[1], [2], [3]] -> [1,2,3]\n",
        "      index_set = sum(cand_index_set[n - 1], [])\n",
        "      # ------------------------------Note3------------------------------- #\n",
        "      n -= 1\n",
        "      \n",
        "      # 控制要mask词的数量（已mask+将要mask） 不超过 num_to_predict\n",
        "      while len(covered_indexes) + len(index_set) > num_to_predict:\n",
        "        if n==0:\n",
        "          break\n",
        "        # 举例，取2-gram， 再尝试\n",
        "        index_set = sum(cand_index_set[n - 1], [])\n",
        "        n -= 1\n",
        "      # 如果 经过 上面的while 仍没找到，说明要重新找\n",
        "      if len(covered_indexes) + len(index_set) > num_to_predict:\n",
        "        continue\n",
        "\n",
        "      is_any_index_covered = False\n",
        "\n",
        "      # 如果该索引已经在covered_indexes（已经选中mask的索引），则不用再次被mask \n",
        "      for index in index_set:\n",
        "        if index in covered_indexes:\n",
        "          is_any_index_covered = True\n",
        "          break\n",
        "      if is_any_index_covered:\n",
        "        continue\n",
        "      # 如果该索引不在covered_indexes（已经选中mask的索引），则需要被mask \n",
        "      for index in index_set:\n",
        "        covered_indexes.add(index)\n",
        "\n",
        "    mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_ids))]\n",
        "    mask_labels += [0] * (max_seq_len - len(mask_labels))\n",
        "    # 上述效果：\n",
        "    # input_ids [1,2,3,4,5,6,7]  7 *0.15=1.05? 需要mask一个\n",
        "    # mask_labels [0,0,1,0,0,0,0]（举例）\n",
        "    return torch.tensor(mask_labels[:max_seq_len])\n",
        "\n",
        "\n",
        "  def ngram_mask(self, input_ids_list, max_seq_len):\n",
        "    mask_labels = []\n",
        "    for i, input_ids in enumerate(input_ids_list):\n",
        "      mask_label = self._ngram_mask(input_ids, max_seq_len, seed = i)\n",
        "      mask_labels.append(mask_label)\n",
        "    return torch.stack(mask_labels, dim = 0)\n",
        "  \n",
        "\n",
        "  def mask_tokens(self, inputs: torch.Tensor, mask_labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # inputs：input_ids\n",
        "    labels = inputs.clone()\n",
        "\n",
        "    probability_matrix = mask_labels\n",
        "    # ---------------------------- 都是 huggingface transformers 源码 ------------------------#\n",
        "    special_tokens_mask = [self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens = True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype = torch.bool), value = 0.0)\n",
        "    masked_indices = probability_matrix.bool()\n",
        "\n",
        "    labels[~masked_indices] = -100\n",
        "    # 80%的词 被替换成tokenizer.mask_token [MASK]\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "    # 10%的词 被随机替换\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(self.tokenizer), labels.shape, dtype = torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "    # ---------------------------- 都是 huggingface transformers 源码 ------------------------#\n",
        "    # 其余 10%的词 不变\n",
        "    return inputs, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    \n",
        "    input_ids, token_type_ids, attention_mask, = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, max_seq_len)\n",
        "\n",
        "    batch_mask = self.ngram_mask(input_ids_list, max_seq_len)\n",
        "    input_ids, mlm_labels = self.mask_tokens(input_ids, batch_mask)                     \n",
        "    \n",
        "    data_dict = {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': mlm_labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "dH2JXAdOqAwE"
      },
      "id": "dH2JXAdOqAwE",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![mask](https://img-blog.csdnimg.cn/6bd70eca797d4b5682baa95f6225bee9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "GNqKLD6gqkff"
      },
      "id": "GNqKLD6gqkff"
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = Collator(max_seq_len = 64, tokenizer = tokenizer, mlm_probability = 0.15)"
      ],
      "metadata": {
        "id": "tQB7hFWkqllP"
      },
      "id": "tQB7hFWkqllP",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "os.path.exists()就是判断括号里的文件是否存在的意思，括号内的可以是文件路径。\n",
        "\n",
        "os.makedirs() 方法用于递归创建目录。\n",
        "```\n",
        "os.makedirs(path, mode=0o777)\n",
        "参数\n",
        "path -- 需要递归创建的目录。\n",
        "mode -- 权限模式。\n",
        "```"
      ],
      "metadata": {
        "id": "TaRwXQLLr5Ou"
      },
      "id": "TaRwXQLLr5Ou"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推荐使用pathlib库\n",
        "\n",
        "相比于老式的 os.path 有几个优势：\n",
        "\n",
        "1.   老的路径操作函数管理比较混乱，有的是导入 os, 有的又是在 os.path 当中，而新的用法统一可以用 pathlib 管理。\n",
        "2.   老用法在处理不同操作系统 win，mac 以及 linux 之间很吃力。换了操作系统常常要改代码，还经常需要进行一些额外操作。\n",
        "3.   老用法主要是函数形式，返回的数据类型通常是字符串。但是路径和字符串并不等价，所以在使用 os 操作路径的时候常常还要引入其他类库协助操作。新用法是面向对象，处理起来更灵活方便。\n",
        "4.   pathlib 简化了很多操作，用起来更轻松。\n",
        "\n",
        "```\n",
        "操作\t     os and os.path\t   pathlib\n",
        "绝对路径\t   os.path.abspath\t   Path.resolve\n",
        "修改权限\t   os.chmod\t       Path.chmod\n",
        "创建目录     os.mkdir\t       Path.mkdir\n",
        "重命名\t    os.rename\t       Path.rename\n",
        "移动\t     os.replace\t      Path.replace\n",
        "删除目录     os.rmdir       \tPath.rmdir\n",
        "删除文件\t   os.remove, os.unlink\t Path.unlink\n",
        "工作目录\t   os.getcwd\t      Path.cwd\n",
        "是否存在\t   os.path.exists\t     Path.exists\n",
        "用户目录\t   os.path.expanduser\t   Path.expanduser and Path.home\n",
        "是否为目录    os.path.isdir\t     Path.is_dir\n",
        "是否为文件    os.path.isfile\t    Path.is_file\n",
        "是否为连接    os.path.islink\t   Path.is_symlink\n",
        "文件属性\t   os.stat\t       Path.stat, Path.owner, Path.group\n",
        "是否为绝对路径\tos.path.isabs\t    PurePath.is_absolute\n",
        "路径拼接\t   os.path.join\t     PurePath.joinpath\n",
        "文件名\t    os.path.basename \t  PurePath.name\n",
        "上级目录\t   os.path.dirname\t  PurePath.parent\n",
        "同名文件\t   os.path.samefile\t   Path.samefile\n",
        "后缀\t     os.path.splitext\t   PurePath.suffix\n",
        "```"
      ],
      "metadata": {
        "id": "xOEvRbqc8ZL2"
      },
      "id": "xOEvRbqc8ZL2"
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dir(path):\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "metadata": {
        "id": "VR4q728OtKfX"
      },
      "id": "VR4q728OtKfX",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = os.path.join(output_dir, 'log')\n",
        "model_save_dir = os.path.join(output_dir, 'best_model_ckpt')\n",
        "tokenizer_and_config = os.path.join(output_dir, 'tokenizer_and_config')\n",
        "\n",
        "check_dir(model_save_dir)\n",
        "check_dir(logging_dir)\n",
        "check_dir(tokenizer_and_config)"
      ],
      "metadata": {
        "id": "Zekym820sUrn"
      },
      "id": "Zekym820sUrn",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=1e-6,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=128,\n",
        "    save_steps=2000,\n",
        "    logging_steps=200,\n",
        "    save_total_limit=5,\n",
        "    fp16=False,\n",
        "    prediction_loss_only=True,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_first_step=True,\n",
        "    dataloader_num_workers=4,\n",
        "    disable_tqdm=False,\n",
        "    seed=2022\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey8waTaxyNC2",
        "outputId": "c625cea2-260f-4adc-90ab-af4a4810be67"
      },
      "id": "Ey8waTaxyNC2",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "#\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "rW8jCTRH1jN-",
        "outputId": "d5ec1dca-eccb-4328-efce-5dc25de5fead"
      },
      "id": "rW8jCTRH1jN-",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 34334\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 269\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='269' max='269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [269/269 12:13, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.679006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.928242</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=269, training_loss=1.889611473757095)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "05_self_Adaptive_Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}