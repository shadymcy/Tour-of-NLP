# BERT

*参考论文:*\
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 	arXiv:1810.04805\
The code and pre-trained models are available at https://github.com/google-research/bert.
