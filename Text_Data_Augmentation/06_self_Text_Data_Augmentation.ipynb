{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwtCjsf4sTwU",
    "outputId": "3023fd8f-27ac-4df3-a67f-cef0935dfebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 12 10:47:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6XY-7WNswSL",
    "outputId": "3e2685db-114b-45b9-b243-068dac87afa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/drive')\n",
    "#设置路径\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqvtiNDuuAxi",
    "outputId": "eed0c76d-ef01-4b78-bb75-c916a3c3a263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting synonyms\n",
      "  Downloading synonyms-3.16.0.tar.gz (10.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.7 MB 4.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (1.1.0)\n",
      "Building wheels for collected packages: synonyms\n",
      "  Building wheel for synonyms (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for synonyms: filename=synonyms-3.16.0-py3-none-any.whl size=10832785 sha256=1ee993a66996743f1dc2f7be2749d057c29d5e19b2571f6d1ce428538b0013fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/cd/43/b4548753509a94471fc946967a07116252d49aeeb689db8f7c\n",
      "Successfully built synonyms\n",
      "Installing collected packages: synonyms\n",
      "Successfully installed synonyms-3.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h10xpanIu7nh",
    "outputId": "684a99f7-2692-4218-a7d2-78c9b0e09780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      ">> Synonyms load wordseg dict [/usr/local/lib/python3.7/dist-packages/synonyms/data/vocab.txt] ... \n",
      ">> Synonyms on loading stopwords [/usr/local/lib/python3.7/dist-packages/synonyms/data/stopwords.txt] ...\n",
      "[Synonyms] on loading vectors [/usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz] ...\n",
      "\n",
      "[Synonyms] downloading data from https://github.com/chatopera/Synonyms/releases/download/3.15.0/words.vector.gz to /usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz ... \n",
      " this only happens if SYNONYMS_WORD2VEC_BIN_URL_ZH_CN is not present and Synonyms initialization for the first time. \n",
      " It would take minutes that depends on network.\n",
      "\n",
      "[Synonyms] downloaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mOaqDkfvPU8",
    "outputId": "609f206d-f7e2-40b0-a5f8-4b34196d5c32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['篮球', '橄榄球', '排球', '棒球', '美式足球', '冰球', '拳击', '网球', '高尔夫球', '高球'],\n",
       " [1.0,\n",
       "  0.81482756,\n",
       "  0.78554475,\n",
       "  0.7845952,\n",
       "  0.7815255,\n",
       "  0.7550466,\n",
       "  0.7411452,\n",
       "  0.7350726,\n",
       "  0.72586256,\n",
       "  0.7199612])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms.nearby('篮球')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE5-bWj3NWHP"
   },
   "source": [
    "# EDA(Easy Data Augmentation)\n",
    "![UDA1](https://img-blog.csdnimg.cn/c5ffcca4482c4c42beb6f1215e37657c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
    "### EDA：用于提高文本分类任务性能的简单数据增强技术。 EDA 由四个简单但功能强大的操作组成：同义词替换、随机插入、随机交换和随机删除。\n",
    "### 之前的工作已经提出了一些 NLP 中数据增强的技术，回译通过将句子翻译成法语然后再翻译成英语来生成新数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnQO5mqsO2BQ"
   },
   "source": [
    "### 对于训练集中的给定句子，随机选择并执行以下操作之一：\n",
    "* 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
    "* 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
    "* 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
    "* 随机删除（RD）：以概率 p 随机删除句子中的每个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3M9EIDDO3Bf"
   },
   "source": [
    "### 停用词 stop word\n",
    "\n",
    "停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words（停用词）\n",
    "```\n",
    "str = \"00000003210Runoob01230000000\"; \n",
    "print str.strip( '0' );  # 去除首尾字符 0\n",
    "\n",
    "str2 = \"   Runoob      \";   # 去除首尾空格\n",
    "print str2.strip();\n",
    "\n",
    "3210Runoob0123\n",
    "Runoob\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LXVB0xAdPdzO"
   },
   "outputs": [],
   "source": [
    "# strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "stop_words = {word.strip() for word in open('/content/drive/MyDrive/Colab Notebooks/baidu_stopwords.txt', 'r', encoding='utf8').readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jUQPnSQXQGEP"
   },
   "outputs": [],
   "source": [
    "# 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
    "import random\n",
    "def get_synonyms(word):\n",
    "  # (['mama'],['0.9'])\n",
    "  #取出元组第0个元素（['mama']），并去重\n",
    "  sys = set(synonyms.nearby(word)[0])\n",
    "  #将word从同义词列表中去除\n",
    "  if word in sys:\n",
    "    sys.remove(word)\n",
    "  return list(sys)\n",
    "  # 如果输入\"给力\" 可能没有同义词（同义词只有他自己） 则返回  ([],[])\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "  new_words = words.copy()\n",
    "  # 遍历句子中的每个词，并且这个词不是停用词\n",
    "  # set()去重， 以列表形式返回\n",
    "  random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "  # 打乱\n",
    "  random.shuffle(random_word_list)\n",
    "  num_replaced = 0\n",
    "  for random_word in random_word_list:\n",
    "    # 得到近义词列表\n",
    "    synonyms = get_synonyms(random_word)\n",
    "    if len(synonyms) >= 1:\n",
    "      # 随机取出一个同义词\n",
    "      synonym = random.choice(list(synonyms))\n",
    "      # 用synonym替换原词\n",
    "      new_words = [synonym if word == random_word else word for word in new_words]\n",
    "      num_replaced += 1\n",
    "    if num_replaced >= n:\n",
    "      break\n",
    "  # new_words 已经是个列表了 下一块代码举例\n",
    "  sentence = ' '.join(new_words)\n",
    "  new_words = sentence.split(' ')\n",
    "  return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbDtk6R6YltF"
   },
   "source": [
    "### 存在一种情况  \n",
    "*有近义词：**actor** -> **film star** 一个单词的近义词是两个单词*\n",
    "\n",
    "sentence = ['in', 'actor']\n",
    "\n",
    "*默认new_words = ['in', 'film star']*\n",
    "\n",
    "*希望有如下表示：*\n",
    "new_words = ['in', 'film', 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5bkLsC96Zn5V"
   },
   "outputs": [],
   "source": [
    "# 随机删除（RD）：以概率 p 随机删除句子中的每个单词。\n",
    "def random_deletion(words, p):\n",
    "  if len(words) == 1:\n",
    "    return words\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    # 概率\n",
    "    r = random.uniform(0, 1)\n",
    "    # r>p 保留\n",
    "    if r > p:\n",
    "      new_words.append(word)\n",
    "  # 如果都删没了 随机返回一个词\n",
    "  if len(new_words) == 0:\n",
    "    random_int = random.randint(0, len(words) - 1)\n",
    "    return [words[random_int]]\n",
    "  return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "547rlZnwbGfO"
   },
   "outputs": [],
   "source": [
    "# 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
    "def swap_word(new_words):\n",
    "  random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "  random_idx_2 = random_idx_1\n",
    "  \n",
    "  count = 0\n",
    "  # 两者相等重新取idx_2\n",
    "  while random_idx_2 == random_idx_1:\n",
    "    random_idx_2 - random.randint(0, len(new_words) - 1)\n",
    "    count += 1\n",
    "    # 取了三次还是相等 立即推\n",
    "    if count > 3:\n",
    "      return new_words\n",
    "\n",
    "  new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "  return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "  new_words = words.copy()\n",
    "  for _ in range(n):\n",
    "    new_words = swap_word(new_words)\n",
    "  return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3j4gTLdNm0La"
   },
   "outputs": [],
   "source": [
    "# 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
    "def add_word(new_words):\n",
    "  # 同义词列表\n",
    "  synonyms = []\n",
    "  count = 0\n",
    "  while len(synonyms) < 1:\n",
    "    # 在句子中随机找到一个词\n",
    "    random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "    synonyms = get_synonyms(random_word)\n",
    "    count += 1\n",
    "    # 找了10次同义词数量仍小于1，立即推\n",
    "    if count >= 10:\n",
    "      return \n",
    "  #将同义词表中第一个插入\n",
    "  random_synonym = synonyms[0]\n",
    "  # 取出要插入的位置  \n",
    "  random_idx = random.randint(0, len(new_words) - 1)\n",
    "  new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def random_insert(words, n):\n",
    "  new_words = words.copy()\n",
    "  for i in range(n):\n",
    "    add_word(new_words)\n",
    "  return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okqcsY-ivdK5",
    "outputId": "16dee6df-2f46-49e1-d25b-a107c1ff52e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '少', '的', '现象'], ['t', 'nr', 'n', 'n', 'uj', 'n', 'n', 'v', 'n', 'a', 'uj', 'n'])\n"
     ]
    }
   ],
   "source": [
    "words = synonyms.seg('目前华为部分型号的手机产品出现货少的现象')\n",
    "print(words)\n",
    "# 后面的是词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Lh7LU9vdqV5a"
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "# EDA方法\n",
    "# 参数num_aug：增加了几条数据\n",
    "def eda(sentence, alpha_sr= 0.1, alpha_ri = 0.1, alpha_rs = 0.1, p_rd = 0.1, num_aug = 9):\n",
    "  words = synonyms.seg(sentence)[0]\n",
    "  num_words = len(words)\n",
    "  augmented_sentence = []\n",
    "  \n",
    "  # 每种技术增加了多少样本\n",
    "  num_new_per_tech = int(num_aug / 4) + 1\n",
    "  # = 3\n",
    "\n",
    "  # 同义词替换数量\n",
    "  n_sr = max(1, int(alpha_sr * num_words))\n",
    "  # 随机插入数量\n",
    "  n_ri = max(1, int(alpha_ri * num_words))\n",
    "  # 随机交换数量\n",
    "  n_rs = max(1, int(alpha_rs * num_words))\n",
    "\n",
    "  # 同义词替换\n",
    "  for i in range(num_new_per_tech):\n",
    "    a_words = synonym_replacement(words, n_sr)\n",
    "    # a_words 是列表 []\n",
    "    print('a-words(同义词替换):',a_words)\n",
    "    augmented_sentence.append(' '.join(a_words))\n",
    "  # 随机插入\n",
    "  for i in range(num_new_per_tech):\n",
    "    a_words = random_insert(words, n_ri)\n",
    "    augmented_sentence.append(' '.join(a_words))\n",
    "  # 随机交换\n",
    "  for i in range(num_new_per_tech):\n",
    "    a_words = random_swap(words, n_rs)\n",
    "    augmented_sentence.append(' '.join(a_words))\n",
    "  # 随机删除\n",
    "  for i in range(num_new_per_tech):\n",
    "    a_words = random_deletion(words, p_rd)\n",
    "    augmented_sentence.append(' '.join(a_words))\n",
    "\n",
    "  shuffle(augmented_sentence)\n",
    "\n",
    "  if num_aug >= 1:\n",
    "    augmented_sentence = augmented_sentence[:num_aug]\n",
    "  else: # num_aug<1\n",
    "    keep_prob = num_aug / len(augmented_sentence)\n",
    "    # random_delete\n",
    "    augmented_sentence = [s for s in augmented_sentence if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "  return augmented_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFV5Q0IP517R",
    "outputId": "54e3b02b-4431-43a4-a032-002657d91826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-words(同义词替换): ['9', '月底', '15', '日', '以来', '，', 'GT5316SB0', '、', '高通', '、', '三星', '等', '华为', '的', '重要', '合作伙伴', '，', '只要', '没有', '美国', '的', '相关', '许可证', '，', '即使', '无法', '配给', '芯片', '给', '华为', '，', '而', '中芯国际', '等', '国产', '芯片', '企业', '，', '也', '因', '采用', '美国', '纳米技术', '，', '而', '无法', '供货', '给', '华为', '。', '目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货品', '少', '的', '现象', '，', '若', '该', '形势', '持续', '下去', '，', '华为', '手机', '投资业务', '将', '遭受', '重创', '。']\n",
      "a-words(同义词替换): ['9', '年初', '15', '日', '以来', '，', '台积电', '、', '高通', '、', '三星', '等', '华为', '的', '重要', '商业伙伴', '，', '只要', '没有', '德国', '的', '上述', '许可证', '，', '就', '无法', '供应', '芯片', '给', '华为', '，', '而', '中芯国际', '等', '换代', '芯片', '企业', '，', '也', '因', '采用', '德国', '技术', '，', '而', '无法', '供货', '给', '华为', '。', '目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '多出', '的', '现象', '，', '若', '该', '形势', '持续', '下去', '，', '华为', '手机', '业务', '将', '遭受', '重创', '。']\n",
      "a-words(同义词替换): ['9', '月', '15', '日', '以来', '，', '富士康', '、', 'AMD', '、', '三星', '等', '华为', '的', '重要', '合作伙伴', '，', '只要', '没有', '美国', '的', '前述', '许可证', '，', '都', '无法', '供应', '微处理器', '给', '华为', '，', '而', '中芯国际', '等', '国产', '微处理器', '企业', '，', '也', '因', '采用', '美国', '关键技术', '，', '而', '无法', '接单', '给', '华为', '。', '目前', '华为', '部分', '型号', '的', '手机', '商品', '出现', '货', '少', '的', '现象', '，', '若', '该', '形势', '持续', '下去', '，', '华为', '手机', '业务', '将', '遭受', '重创', '。']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月底 15 日 以来 ， GT5316SB0 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 即使 无法 配给 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 纳米技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货品 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 投资业务 将 遭受 重创 。',\n",
       " '将本 9 笔记本 月 伊始 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 混动 给 华为 ， 而 中芯国际 等 国产 伊始 芯片 企业 ， 也 因 采用 美国 技术 ， 而 机种 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 持续性 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 只要 没有 美国 的 相关 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 产品 出现 货 的 现象 ， 若 该 形势 下去 ， 手机 业务 遭受 重创',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 ， 都 无法 供应 芯片 给 华为 ， 中芯国际 等 国产 芯片 企业 ， 也 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 持续 下去 ， 华为 手机 业务 将 遭受 重创',\n",
       " '9 月 15 日 以来 ， 富士康 、 AMD 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 前述 许可证 ， 都 无法 供应 微处理器 给 华为 ， 而 中芯国际 等 国产 微处理器 企业 ， 也 因 采用 美国 关键技术 ， 而 无法 接单 给 华为 。 目前 华为 部分 型号 的 手机 商品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
       " '9 年初 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 商业伙伴 ， 只要 没有 德国 的 上述 许可证 ， 就 无法 供应 芯片 给 华为 ， 而 中芯国际 等 换代 芯片 企业 ， 也 因 采用 德国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 多出 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYUvgIjRujZf"
   },
   "source": [
    "## 闭包数据增强\n",
    "数据集中每条数据有两个句子 \\\n",
    "a, b, 1\\\n",
    "a, c, 1\\\n",
    "a, d, 0\\\n",
    "a-b(相似), a-c => b-c\\\n",
    "a-b, ad不相似 => bd不相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LYi_3K-ktxJf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "def parse_data(path, data_type='train'):\n",
    "  sentence_a = []\n",
    "  sentence_b = []\n",
    "  labels = []\n",
    "\n",
    "  with open(path, 'r', encoding = 'utf8') as f:\n",
    "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
    "      line = json.loads(line)\n",
    "      sentence_a.append(line['sentence1'])\n",
    "      sentence_b.append(line['sentence2'])\n",
    "      if data_type != 'test':\n",
    "        labels.append(int(line['label']))\n",
    "      else:\n",
    "        labels.append(0)\n",
    "\n",
    "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSYW4V-vw5G3",
    "outputId": "860add4a-b0e1-4c4b-d1a4-e8bd82c4288b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 221927.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = parse_data('/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zyeDVsEo1uJ3",
    "outputId": "6e3087f7-b99c-472e-aec2-81911ddb2947"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5f32e007-dc0f-4c38-a0a3-0f4d36d24fed\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
       "      <td>借呗有先息到期还本吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蚂蚁花呗说我违约一次</td>\n",
       "      <td>蚂蚁花呗违约行为是什么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
       "      <td>下月花呗账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
       "      <td>借呗得评估多久</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的花呗账单是***，还款怎么是***</td>\n",
       "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f32e007-dc0f-4c38-a0a3-0f4d36d24fed')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5f32e007-dc0f-4c38-a0a3-0f4d36d24fed button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5f32e007-dc0f-4c38-a0a3-0f4d36d24fed');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                text_a                                 text_b  labels\n",
       "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
       "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
       "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
       "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
       "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g84ecFQM2TjP",
    "outputId": "280e1ad3-14d3-4a70-f836-fa09d62ef4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型: <class 'tuple'>\n",
      "数据长度: 2\n",
      "data[0]: 为什么我开通不了花呗\n",
      "-------\n",
      "data[1]:            text_a                            text_b  labels\n",
      "21     为什么我开通不了花呗  我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以       0\n",
      "1333   为什么我开通不了花呗                        为什么不可以开通花呗       0\n",
      "17994  为什么我开通不了花呗                          我为何打不开花呗       0\n",
      "18096  为什么我开通不了花呗                         我开通不了蚂蚁花呗       1\n",
      "19484  为什么我开通不了花呗           你直接帮我查一下 确实开通不了花呗吗？我的账户       1\n",
      "31930  为什么我开通不了花呗                         电脑端怎么开通花呗       0\n",
      "-------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6 entries, 21 to 31930\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text_a  6 non-null      object\n",
      " 1   text_b  6 non-null      object\n",
      " 2   labels  6 non-null      int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 192.0+ bytes\n",
      "data[1].info(): None\n"
     ]
    }
   ],
   "source": [
    "for data in train_df.groupby(by = ['text_a']):\n",
    "  if len(data[1])>=3:\n",
    "   \n",
    "    print('数据类型:',type(data))\n",
    "    print('数据长度:',len(data))\n",
    "    print('data[0]:',data[0]) # text_a句子\n",
    "    print('-------')\n",
    "    print('data[1]:',data[1]) # 以text_a为主键返回的dataFrame\n",
    "    print('-------')\n",
    "  \n",
    "    print('data[1].info():',data[1].info())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举例\n",
    "a : 为什么我开通不了花呗 b: 我一直想买苹果 p，没钱，想分期付款，除了花呗，还有什么可以 c:我开通不了蚂蚁花呗 ab 不相似， ac相似，=》bc不相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hnHJ2iJa7OBW"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def aug_group_by_a(df):\n",
    "  aug_data = defaultdict(list)\n",
    "  # 以text_a中的句子为g\n",
    "  for g, data in df.groupby(by = ['text_a']):\n",
    "    # 如果只有一条数据 无法数据增强\n",
    "    if len(data) < 2:\n",
    "      continue\n",
    "    # iloc[ : , : ] 行列切片以“，”隔开，前面的冒号就是取行数，后面的冒号是取列数\n",
    "    for i in range(len(data)):\n",
    "      for j in range(i+1, len(data)):\n",
    "        # 取出b的值，a,b的label\n",
    "        row_i_text = data.iloc[i, 1]\n",
    "        row_i_label = data.iloc[i, 2]\n",
    "        # 取出c的值，a,c的label\n",
    "        row_j_text = data.iloc[j, 1]\n",
    "        row_j_label = data.iloc[j, 2]\n",
    "        # 如果 ab ， ac都不相似则不考虑\n",
    "        if row_i_label == row_j_label == 0:\n",
    "          continue\n",
    "        # ab相似， ac也相似， bc就相似\n",
    "        aug_label = 1 if row_i_label == row_j_label == 1 else 0\n",
    "\n",
    "        aug_data['text_a'].append(row_i_text)\n",
    "        aug_data['text_b'].append(row_j_text)\n",
    "        aug_data['labels'].append(aug_label)\n",
    "  return pd.DataFrame(aug_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9po58_K8qg3",
    "outputId": "4cddb30c-fb4e-4175-f7dc-bb96eb70e943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text_a                   text_b  labels\n",
      "0    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以                我开通不了蚂蚁花呗       0\n",
      "1    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
      "2                          为什么不可以开通花呗                我开通不了蚂蚁花呗       0\n",
      "3                          为什么不可以开通花呗  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
      "4                            我为何打不开花呗                我开通不了蚂蚁花呗       0\n",
      "..                                ...                      ...     ...\n",
      "223                           你是人工服务吗                   需求人工客服       0\n",
      "224                           你是人工服务吗                   联系人工客服       1\n",
      "225                            需求人工客服                   联系人工客服       0\n",
      "226                     身份证过期可以用蚂蚁借呗吗           身份证过期可以注册蚂蚁借呗吗       0\n",
      "227                            人工关闭花呗     一个帐户的花呗关，另一个帐户的花呗怎么开       0\n",
      "\n",
      "[228 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "aug_train_a = aug_group_by_a(train_df)\n",
    "# 看增强了多少条数据\n",
    "print(aug_train_a)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_self_Text_Data_Augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
