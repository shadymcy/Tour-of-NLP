{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwtCjsf4sTwU",
        "outputId": "9b900a57-d4b6-488a-ee57-a3bedf8d4cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 15 05:58:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6XY-7WNswSL",
        "outputId": "2990f181-7f13-49a4-9058-34b01e92bdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqvtiNDuuAxi",
        "outputId": "c2551cbd-cfc4-4c24-f5a7-1e28bf60e57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting synonyms\n",
            "  Downloading synonyms-3.16.0.tar.gz (10.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (3.1.0)\n",
            "Building wheels for collected packages: synonyms\n",
            "  Building wheel for synonyms (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for synonyms: filename=synonyms-3.16.0-py3-none-any.whl size=10832785 sha256=ef2a2ba06c79feab723ce78d8700e503ec8531220a57db17b1ec2c6ca686f2c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cd/43/b4548753509a94471fc946967a07116252d49aeeb689db8f7c\n",
            "Successfully built synonyms\n",
            "Installing collected packages: synonyms\n",
            "Successfully installed synonyms-3.16.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -U synonyms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KiToP8FDIuC",
        "outputId": "728789be-405a-478f-a67a-4b2ea76fa4e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Cl5K2sDNPT",
        "outputId": "2fb9db2b-3823-490c-cfef-a4c846098f00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 25.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 31.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=cd9f77627587a5c9b8a46bbb3bf99d6545d1a7f997d98d70836be71a91b0e1db\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h10xpanIu7nh",
        "outputId": "8d65b8f4-5364-4fb8-cf65-9991a313384d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jieba] default dict file path ../data/vocab.txt\n",
            "[jieba] default dict file path ../data/vocab.txt\n",
            "[jieba] load default dict ../data/vocab.txt ...\n",
            "[jieba] load default dict ../data/vocab.txt ...\n",
            ">> Synonyms load wordseg dict [/usr/local/lib/python3.7/dist-packages/synonyms/data/vocab.txt] ... \n",
            ">> Synonyms on loading stopwords [/usr/local/lib/python3.7/dist-packages/synonyms/data/stopwords.txt] ...\n",
            "[Synonyms] on loading vectors [/usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz] ...\n",
            "\n",
            "[Synonyms] downloading data from https://github.com/chatopera/Synonyms/releases/download/3.15.0/words.vector.gz to /usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz ... \n",
            " this only happens if SYNONYMS_WORD2VEC_BIN_URL_ZH_CN is not present and Synonyms initialization for the first time. \n",
            " It would take minutes that depends on network.\n",
            "\n",
            "[Synonyms] downloaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import synonyms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mOaqDkfvPU8",
        "outputId": "30156ef7-4022-49b7-b900-929cc15a6070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['篮球', '橄榄球', '排球', '棒球', '美式足球', '冰球', '拳击', '网球', '高尔夫球', '高球'],\n",
              " [1.0,\n",
              "  0.81482756,\n",
              "  0.78554475,\n",
              "  0.7845952,\n",
              "  0.7815255,\n",
              "  0.7550466,\n",
              "  0.7411452,\n",
              "  0.7350726,\n",
              "  0.72586256,\n",
              "  0.7199612])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "synonyms.nearby('篮球')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE5-bWj3NWHP"
      },
      "source": [
        "# EDA(Easy Data Augmentation)\n",
        "![UDA1](https://img-blog.csdnimg.cn/c5ffcca4482c4c42beb6f1215e37657c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "### EDA：用于提高文本分类任务性能的简单数据增强技术。 EDA 由四个简单但功能强大的操作组成：同义词替换、随机插入、随机交换和随机删除。\n",
        "### 之前的工作已经提出了一些 NLP 中数据增强的技术，回译通过将句子翻译成法语然后再翻译成英语来生成新数据。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnQO5mqsO2BQ"
      },
      "source": [
        "### 对于训练集中的给定句子，随机选择并执行以下操作之一：\n",
        "* 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
        "* 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
        "* 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
        "* 随机删除（RD）：以概率 p 随机删除句子中的每个单词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3M9EIDDO3Bf"
      },
      "source": [
        "### 停用词 stop word\n",
        "\n",
        "停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words（停用词）\n",
        "```\n",
        "str = \"00000003210Runoob01230000000\"; \n",
        "print str.strip( '0' );  # 去除首尾字符 0\n",
        "\n",
        "str2 = \"   Runoob      \";   # 去除首尾空格\n",
        "print str2.strip();\n",
        "\n",
        "3210Runoob0123\n",
        "Runoob\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LXVB0xAdPdzO"
      },
      "outputs": [],
      "source": [
        "# strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
        "stop_words = {word.strip() for word in open('/content/drive/MyDrive/Colab Notebooks/dataset/baidu_stopwords.txt', 'r', encoding='utf8').readlines()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jUQPnSQXQGEP"
      },
      "outputs": [],
      "source": [
        "# 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
        "import random\n",
        "def get_synonyms(word):\n",
        "  # (['mama'],['0.9'])\n",
        "  #取出元组第0个元素（['mama']），并去重\n",
        "  sys = set(synonyms.nearby(word)[0])\n",
        "  #将word从同义词列表中去除\n",
        "  if word in sys:\n",
        "    sys.remove(word)\n",
        "  return list(sys)\n",
        "  # 如果输入\"给力\" 可能没有同义词（同义词只有他自己） 则返回  ([],[])\n",
        "\n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "  new_words = words.copy()\n",
        "  # 遍历句子中的每个词，并且这个词不是停用词\n",
        "  # set()去重， 以列表形式返回\n",
        "  random_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "  # 打乱\n",
        "  random.shuffle(random_word_list)\n",
        "  num_replaced = 0\n",
        "  for random_word in random_word_list:\n",
        "    # 得到近义词列表\n",
        "    synonyms = get_synonyms(random_word)\n",
        "    if len(synonyms) >= 1:\n",
        "      # 随机取出一个同义词\n",
        "      synonym = random.choice(list(synonyms))\n",
        "      # 用synonym替换原词\n",
        "      new_words = [synonym if word == random_word else word for word in new_words]\n",
        "      num_replaced += 1\n",
        "    if num_replaced >= n:\n",
        "      break\n",
        "  # new_words 已经是个列表了 下一块代码举例\n",
        "  sentence = ' '.join(new_words)\n",
        "  new_words = sentence.split(' ')\n",
        "  return new_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDtk6R6YltF"
      },
      "source": [
        "### 存在一种情况  \n",
        "*有近义词：**actor** -> **film star** 一个单词的近义词是两个单词*\n",
        "\n",
        "sentence = ['in', 'actor']\n",
        "\n",
        "*默认new_words = ['in', 'film star']*\n",
        "\n",
        "*希望有如下表示：*\n",
        "new_words = ['in', 'film', 'star']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5bkLsC96Zn5V"
      },
      "outputs": [],
      "source": [
        "# 随机删除（RD）：以概率 p 随机删除句子中的每个单词。\n",
        "def random_deletion(words, p):\n",
        "  if len(words) == 1:\n",
        "    return words\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    # 概率\n",
        "    r = random.uniform(0, 1)\n",
        "    # r>p 保留\n",
        "    if r > p:\n",
        "      new_words.append(word)\n",
        "  # 如果都删没了 随机返回一个词\n",
        "  if len(new_words) == 0:\n",
        "    random_int = random.randint(0, len(words) - 1)\n",
        "    return [words[random_int]]\n",
        "  return new_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "547rlZnwbGfO"
      },
      "outputs": [],
      "source": [
        "# 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
        "def swap_word(new_words):\n",
        "  random_idx_1 = random.randint(0, len(new_words) - 1)\n",
        "  random_idx_2 = random_idx_1\n",
        "  \n",
        "  count = 0\n",
        "  # 两者相等重新取idx_2\n",
        "  while random_idx_2 == random_idx_1:\n",
        "    random_idx_2 - random.randint(0, len(new_words) - 1)\n",
        "    count += 1\n",
        "    # 取了三次还是相等 立即推\n",
        "    if count > 3:\n",
        "      return new_words\n",
        "\n",
        "  new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
        "  return new_words\n",
        "\n",
        "def random_swap(words, n):\n",
        "  new_words = words.copy()\n",
        "  for _ in range(n):\n",
        "    new_words = swap_word(new_words)\n",
        "  return new_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3j4gTLdNm0La"
      },
      "outputs": [],
      "source": [
        "# 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
        "def add_word(new_words):\n",
        "  # 同义词列表\n",
        "  synonyms = []\n",
        "  count = 0\n",
        "  while len(synonyms) < 1:\n",
        "    # 在句子中随机找到一个词\n",
        "    random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
        "    synonyms = get_synonyms(random_word)\n",
        "    count += 1\n",
        "    # 找了10次同义词数量仍小于1，立即推\n",
        "    if count >= 10:\n",
        "      return \n",
        "  #将同义词表中第一个插入\n",
        "  random_synonym = synonyms[0]\n",
        "  # 取出要插入的位置  \n",
        "  random_idx = random.randint(0, len(new_words) - 1)\n",
        "  new_words.insert(random_idx, random_synonym)\n",
        "\n",
        "def random_insert(words, n):\n",
        "  new_words = words.copy()\n",
        "  for i in range(n):\n",
        "    add_word(new_words)\n",
        "  return new_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okqcsY-ivdK5",
        "outputId": "6a69e1d0-af64-4719-f290-6dc698daf1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['目前', '华为', '部分', '型号', '的', '手机', '产品', '出现', '货', '少', '的', '现象'], ['t', 'nr', 'n', 'n', 'uj', 'n', 'n', 'v', 'n', 'a', 'uj', 'n'])\n"
          ]
        }
      ],
      "source": [
        "words = synonyms.seg('目前华为部分型号的手机产品出现货少的现象')\n",
        "print(words)\n",
        "# 后面的是词性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Lh7LU9vdqV5a"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "# EDA方法\n",
        "# 参数num_aug：增加了几条数据\n",
        "def eda(sentence, alpha_sr= 0.1, alpha_ri = 0.1, alpha_rs = 0.1, p_rd = 0.1, num_aug = 9):\n",
        "  words = synonyms.seg(sentence)[0]\n",
        "  num_words = len(words)\n",
        "  augmented_sentence = []\n",
        "  \n",
        "  # 每种技术增加了多少样本\n",
        "  num_new_per_tech = int(num_aug / 4) + 1\n",
        "  # = 3\n",
        "\n",
        "  # 同义词替换数量\n",
        "  n_sr = max(1, int(alpha_sr * num_words))\n",
        "  # 随机插入数量\n",
        "  n_ri = max(1, int(alpha_ri * num_words))\n",
        "  # 随机交换数量\n",
        "  n_rs = max(1, int(alpha_rs * num_words))\n",
        "\n",
        "  # 同义词替换\n",
        "  for i in range(num_new_per_tech):\n",
        "    a_words = synonym_replacement(words, n_sr)\n",
        "    # a_words 是列表 []\n",
        "    print('a-words(同义词替换):',a_words)\n",
        "    augmented_sentence.append(' '.join(a_words))\n",
        "  # 随机插入\n",
        "  for i in range(num_new_per_tech):\n",
        "    a_words = random_insert(words, n_ri)\n",
        "    augmented_sentence.append(' '.join(a_words))\n",
        "  # 随机交换\n",
        "  for i in range(num_new_per_tech):\n",
        "    a_words = random_swap(words, n_rs)\n",
        "    augmented_sentence.append(' '.join(a_words))\n",
        "  # 随机删除\n",
        "  for i in range(num_new_per_tech):\n",
        "    a_words = random_deletion(words, p_rd)\n",
        "    augmented_sentence.append(' '.join(a_words))\n",
        "\n",
        "  shuffle(augmented_sentence)\n",
        "\n",
        "  if num_aug >= 1:\n",
        "    augmented_sentence = augmented_sentence[:num_aug]\n",
        "  else: # num_aug<1\n",
        "    keep_prob = num_aug / len(augmented_sentence)\n",
        "    # random_delete\n",
        "    augmented_sentence = [s for s in augmented_sentence if random.uniform(0, 1) < keep_prob]\n",
        "\n",
        "  return augmented_sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFV5Q0IP517R",
        "outputId": "e6c1d8d2-5748-46af-ca79-873145598b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a-words(同义词替换): ['9', '月前', '15', '日', '以来', '，', '台积电', '、', '高通', '、', 'LG', '等', 'OPPO', '的', '重要', '合作伙伴', '，', '只要', '没有', '哥伦比亚', '的', '相关', '许可证', '，', '都', '无法', '供应', '芯片', '给', 'OPPO', '，', '而', '中芯国际', '等', '国产', '芯片', '企业', '，', '也', '因', '采用', '哥伦比亚', '技术', '，', '而', '无法', '供货', '给', 'OPPO', '。', '目前', 'OPPO', '部分', '型号', '的', '手机', '品类', '出现', '货', '少', '的', '现象', '，', '若', '该', '形势', '急剧', '下去', '，', 'OPPO', '手机', '产品销售', '将', '遭受', '重创', '。']\n",
            "a-words(同义词替换): ['9', '月', '15', '日', '以来', '，', '台积电', '、', 'NVIDIA', '、', '三星', '等', '华为', '的', '重要', '合作伙伴', '，', '只要', '没有', '美国', '的', '相关', '许可证', '，', '都', '无法', '供给量', '芯片', '给', '华为', '，', '而', '中芯国际', '等', '国产', '芯片', '企业', '，', '也', '因', '采用', '美国', '技术', '，', '而', '无法', '供货', '给', '华为', '。', '目前', '华为', '部分', '型号', '的', '手机', '家电产品', '出现', '货', '少', '的', '不良现象', '，', '若', '该', '情势', '持续', '下去', '，', '华为', '手机', '投资业务', '将', '遭致', '重创', '。']\n",
            "a-words(同义词替换): ['9', '月', '15', '日', '以来', '，', 'GT5316SB0', '、', '德州仪器', '、', '三星', '等', '华为', '的', '重要', '合作伙伴', '，', '只要', '没有', '美国', '的', '相关', '许可证', '，', '都', '无法', '供应', '芯片', '给', '华为', '，', '而', '中芯国际', '等', '国产', '芯片', '企业', '，', '也', '因', '采用', '美国', '技术', '，', '而', '无法', 'OEM', '给', '华为', '。', '目前', '华为', '部分', '改进型', '的', 'iPhone4', '产品', '出现', '货', '少', '的', '现象', '，', '若', '该', '形势', '持续', '下去', '，', '华为', 'iPhone4', '金融业务', '将', '遭受', '重挫', '。']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 高通 、 三星 华为 的 重要 ， 只要 没有 美国 的 相关 许可证 ， 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 该 形势 持续 下去 ， 华为 手机 业务 。',\n",
              " '9 月 15 日 以来 由于 ， 台积电 国际形势 、 高通 、 三星 等 华为 的 新手机 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 闪存 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 国产化 现象 ， 若 甚至于 该 形势 持续 下去 甚至于 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 NVIDIA 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供给量 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 家电产品 出现 货 少 的 不良现象 ， 若 该 情势 持续 下去 ， 华为 手机 投资业务 将 遭致 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 各样 ， 台积电 、 高通 、 三星 等 华为 的 SE9 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 金融机构 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 至今 无法 供货 给 倘若 华为 。 但此 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 SE9 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月前 15 日 以来 ， 台积电 、 高通 、 LG 等 OPPO 的 重要 合作伙伴 ， 只要 没有 哥伦比亚 的 相关 许可证 ， 都 无法 供应 芯片 给 OPPO ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 哥伦比亚 技术 ， 而 无法 供货 给 OPPO 。 目前 OPPO 部分 型号 的 手机 品类 出现 货 少 的 现象 ， 若 该 形势 急剧 下去 ， OPPO 手机 产品销售 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， GT5316SB0 、 德州仪器 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 OEM 给 华为 。 目前 华为 部分 改进型 的 iPhone4 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 iPhone4 金融业务 将 遭受 重挫 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYUvgIjRujZf"
      },
      "source": [
        "## 闭包数据增强\n",
        "数据集中每条数据有两个句子 \\\n",
        "a, b, 1\\\n",
        "a, c, 1\\\n",
        "a, d, 0\\\n",
        "a-b(相似), a-c => b-c\\\n",
        "a-b, ad不相似 => bd不相似"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LYi_3K-ktxJf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSYW4V-vw5G3",
        "outputId": "04ca838f-418c-4028-a851-8af0901c04d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 122871.81it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df = parse_data('/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json', data_type='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zyeDVsEo1uJ3",
        "outputId": "583818fe-305f-4989-a4a5-3b343ef9a6fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                text_a                                 text_b  labels\n",
              "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
              "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
              "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
              "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
              "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-980e3ddf-7130-408c-8275-12c08c579a3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
              "      <td>借呗有先息到期还本吗</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>蚂蚁花呗说我违约一次</td>\n",
              "      <td>蚂蚁花呗违约行为是什么</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
              "      <td>下月花呗账单</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
              "      <td>借呗得评估多久</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我的花呗账单是***，还款怎么是***</td>\n",
              "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-980e3ddf-7130-408c-8275-12c08c579a3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-980e3ddf-7130-408c-8275-12c08c579a3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-980e3ddf-7130-408c-8275-12c08c579a3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g84ecFQM2TjP",
        "outputId": "71626960-b101-48ba-b7ca-129ea2ebc1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据类型: <class 'tuple'>\n",
            "数据长度: 2\n",
            "data[0]: 为什么我开通不了花呗\n",
            "-------\n",
            "data[1]:            text_a                            text_b  labels\n",
            "21     为什么我开通不了花呗  我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以       0\n",
            "1333   为什么我开通不了花呗                        为什么不可以开通花呗       0\n",
            "17994  为什么我开通不了花呗                          我为何打不开花呗       0\n",
            "18096  为什么我开通不了花呗                         我开通不了蚂蚁花呗       1\n",
            "19484  为什么我开通不了花呗           你直接帮我查一下 确实开通不了花呗吗？我的账户       1\n",
            "31930  为什么我开通不了花呗                         电脑端怎么开通花呗       0\n",
            "-------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6 entries, 21 to 31930\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text_a  6 non-null      object\n",
            " 1   text_b  6 non-null      object\n",
            " 2   labels  6 non-null      int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 192.0+ bytes\n",
            "data[1].info(): None\n"
          ]
        }
      ],
      "source": [
        "for data in train_df.groupby(by = ['text_a']):\n",
        "  if len(data[1])>=3:\n",
        "   \n",
        "    print('数据类型:',type(data))\n",
        "    print('数据长度:',len(data))\n",
        "    print('data[0]:',data[0]) # text_a句子\n",
        "    print('-------')\n",
        "    print('data[1]:',data[1]) # 以text_a为主键返回的dataFrame\n",
        "    print('-------')\n",
        "  \n",
        "    print('data[1].info():',data[1].info())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91fRL87k_TUW"
      },
      "source": [
        "### 举例\n",
        "a : 为什么我开通不了花呗 b: 我一直想买苹果 p，没钱，想分期付款，除了花呗，还有什么可以 c:我开通不了蚂蚁花呗 ab 不相似， ac相似，=》bc不相似"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hnHJ2iJa7OBW"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def aug_group_by_a(df):\n",
        "  aug_data = defaultdict(list)\n",
        "  # 以text_a中的句子为g\n",
        "  for g, data in df.groupby(by = ['text_a']):\n",
        "    # 如果只有一条数据 无法数据增强\n",
        "    if len(data) < 2:\n",
        "      continue\n",
        "    # iloc[ : , : ] 行列切片以“，”隔开，前面的冒号就是取行数，后面的冒号是取列数\n",
        "    for i in range(len(data)):\n",
        "      for j in range(i+1, len(data)):\n",
        "        # 取出b的值，a,b的label\n",
        "        row_i_text = data.iloc[i, 1]\n",
        "        row_i_label = data.iloc[i, 2]\n",
        "        # 取出c的值，a,c的label\n",
        "        row_j_text = data.iloc[j, 1]\n",
        "        row_j_label = data.iloc[j, 2]\n",
        "        # 如果 ab ， ac都不相似则不考虑\n",
        "        if row_i_label == row_j_label == 0:\n",
        "          continue\n",
        "        # ab相似， ac也相似， bc就相似\n",
        "        aug_label = 1 if row_i_label == row_j_label == 1 else 0\n",
        "\n",
        "        aug_data['text_a'].append(row_i_text)\n",
        "        aug_data['text_b'].append(row_j_text)\n",
        "        aug_data['labels'].append(aug_label)\n",
        "  return pd.DataFrame(aug_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9po58_K8qg3",
        "outputId": "646c9f25-de8f-46a0-a18d-f11a862ca2c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               text_a                   text_b  labels\n",
            "0    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以                我开通不了蚂蚁花呗       0\n",
            "1    我一直想买苹果***p，没钱，想分期付款，除了花呗，还有什么可以  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
            "2                          为什么不可以开通花呗                我开通不了蚂蚁花呗       0\n",
            "3                          为什么不可以开通花呗  你直接帮我查一下 确实开通不了花呗吗？我的账户       0\n",
            "4                            我为何打不开花呗                我开通不了蚂蚁花呗       0\n",
            "..                                ...                      ...     ...\n",
            "223                           你是人工服务吗                   需求人工客服       0\n",
            "224                           你是人工服务吗                   联系人工客服       1\n",
            "225                            需求人工客服                   联系人工客服       0\n",
            "226                     身份证过期可以用蚂蚁借呗吗           身份证过期可以注册蚂蚁借呗吗       0\n",
            "227                            人工关闭花呗     一个帐户的花呗关，另一个帐户的花呗怎么开       0\n",
            "\n",
            "[228 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "aug_train_a = aug_group_by_a(train_df)\n",
        "# 看增强了多少条数据\n",
        "print(aug_train_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UDA（Unsupervised Data Augmentation for Consistency Training）用于一致性训练的无监督数据增强\n",
        " ![UDA1](https://img-blog.csdnimg.cn/c9cb603261a1497c8093ee669b7923f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "kLXQuzEl_cP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 什么是一致性训练？\n",
        "### 数据增强 是 创建 逼近 真实的训练数据，并且不改变其标签\n",
        "### 举例： a, b, 1  a c, 1 -> b, c, 1\n",
        "### 一致性训练：增强前和增强后的标签 应该保持一致，利用这个特性训练"
      ],
      "metadata": {
        "id": "ZxgwQy4N_1Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![unsupervised data augmentation](https://img-blog.csdnimg.cn/86f3aad1042b4bf381d662e9c3b48f0a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "Fbzf5v5_DBrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 半监督学习利用无标签样本增强模型鲁棒性\n",
        " * 给定输入x, 计算输出分布$p_{\\theta}(y|x)$, 同时，给输入x进行数据增强，计算出分布$p_{\\theta}(y|\\hat x)$.\n",
        " * 给两个分布计算KL散度"
      ],
      "metadata": {
        "id": "eBpEE_6nCvLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![UDA5](https://img-blog.csdnimg.cn/9d10da70d1d0467e93ef5bb1267ac87f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "UQvETdbrBenT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解释上图\n",
        "![code](https://img-blog.csdnimg.cn/d97f35fd41e0485185f40d50f4fd8e8d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "\n",
        "**ps： unsup_x--(a,b)成为 no_grad_data,不需要反向传播** \\\n",
        "*sup:监督 unsup:无监督*"
      ],
      "metadata": {
        "id": "zDB0XgtBDWYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bucket_sampler import SortedSampler, BucketBatchSampler\n",
        "from EMA import *"
      ],
      "metadata": {
        "id": "HAOjofjfGdKT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "config = {\n",
        "        'train_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "        'dev_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "        'test_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "        'output_path': '.',\n",
        "        'model_path': '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model',\n",
        "        'batch_size': 16,\n",
        "        'num_epochs': 1,\n",
        "        'max_seq_len': 64,\n",
        "        'learning_rate': 2e-5,\n",
        "        'weight_decay': 0.01,\n",
        "        'use_bucket': True,\n",
        "        'bucket_multiplier': 200,\n",
        "        'unsup_data_ratio': 1.5,\n",
        "        'uda_softmax_temp': 0.4,\n",
        "        'uda_confidence_threshold': 0.8,\n",
        "        'device': 'cuda',\n",
        "        'n_gpus': 0,\n",
        "        'logging_step': 300,\n",
        "        'ema_start_step': 500,\n",
        "        'ema_start': False,\n",
        "        'seed': 2022\n",
        "    }\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    config['device'] = 'cpu'\n",
        "else:\n",
        "    config['n_gpus'] = torch.cuda.device_count()\n",
        "    config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DISLVIJEG4Cj",
        "outputId": "d97c87a7-65e9-4447-8cc9-df737b478597"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
      ],
      "metadata": {
        "id": "3rVKA8xzIcZD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True, return_token_type_ids =True, return_attention_mask = True)\n",
        "\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)"
      ],
      "metadata": {
        "id": "L5TtMUNKmp7j"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  对偶数据增强\n",
        "sentence1: a\\\n",
        "sentence2: b\\\n",
        "a, b, 1 变成 b, a, 1"
      ],
      "metadata": {
        "id": "wKJOUy9FpRo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 无监督BERT输入\n",
        "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  # 左右\n",
        "  lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True, return_token_type_ids = True, return_attention_mask = True)\n",
        "  # 右左\n",
        "  rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens = True, return_token_type_ids = True, return_attention_mask = True)\n",
        "\n",
        "  # 元组的形式\n",
        "  inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
        "  inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'], rl_inputs_dict['token_type_ids']))\n",
        "  inputs['attention_mask'].append((lr_inputs_dict['attention_mask'], rl_inputs_dict['attention_mask']))\n",
        "  inputs['labels'].append(label)\n"
      ],
      "metadata": {
        "id": "4NVd4iSqodRr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type = 'train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type = 'dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type = 'test')\n",
        "\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "  processed_data = {}\n",
        "  unsup_data = defaultdict(list)\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "    if data_type == 'train':\n",
        "      reversed_inputs = defaultdict(list)\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total = len(df)):\n",
        "      label = 0 if data_type == 'test' else row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "      if data_type.startswith('test'):\n",
        "        build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
        "\n",
        "\n",
        "      build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "  processed_data['unsup_data'] = unsup_data\n",
        "  return processed_data\n",
        "\n",
        "# processed_data\n",
        "# {\n",
        "#    'train':,\n",
        "#    'dev':,\n",
        "#    'test':,\n",
        "#    'unsup_data':   # 数据量最大的\n",
        "# }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_vphmq44wKQb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data(config, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-8IxyClCIkT",
        "outputId": "5227bc7b-7938-4699-e79d-d58ce4ed63bc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 275405.31it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 190317.46it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 193400.62it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:46<00:00, 739.50it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:05<00:00, 754.37it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:06<00:00, 586.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx],\n",
        "         self.data_dict['labels'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "a7GTURcvu-QA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Collator:\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype = torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype = torch.long)\n",
        "        token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "        attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "\n",
        "      else:\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len - 1] + [self.tokenizer.sep_token_id], dtype = torch.long)\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype = torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype = torch.long)\n",
        "\n",
        "      labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "      return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "    data_dict = {\n",
        "        'input_ids':input_ids,\n",
        "        'token_type_ids':token_type_ids,\n",
        "        'attention_mask':attention_mask,\n",
        "        'labels':labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "UYQSdV9Wwiho"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ],
      "metadata": {
        "id": "gyz3hzqlCeaP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UDA 无监督重新构造dataset 和 collator\n",
        "class UnsupAFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(UnsupAFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    input_ids = self.data_dict['input_ids'][idx]\n",
        "    token_type_ids = self.data_dict['token_type_ids'][idx]\n",
        "    attention_mask = self.data_dict['attention_mask'][idx]\n",
        "    labels = self.data_dict['labels'][idx]\n",
        "    # input_ids[0]：lr_inputs_dict['input_ids']    (build_unsup_bert_inputs)\n",
        "    # input_ids[1]：rl_inputs_dict['input_ids']\n",
        "    return (input_ids[0], token_type_ids[0], attention_mask[0],\n",
        "         input_ids[1], token_type_ids[1], attention_mask[1],\n",
        "         labels)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "44ByblIdChtN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupCollator(Collator):\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    # 根据UnsupAFQMCDataset的getitem 有七个数据\n",
        "    (ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list,\n",
        "     ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list,\n",
        "     labels_list) = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in ab_input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    # 分批整ab, ba（填充与截断）\n",
        "    ab_input_ids, ab_token_type_ids, ab_attention_mask, labels = self.pad_and_truncate(ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, labels_list, max_seq_len)\n",
        "    ba_input_ids, ba_token_type_ids, ba_attention_mask, labels = self.pad_and_truncate(ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "\n",
        "    data_dict = {\n",
        "        'ab_input_ids':ab_input_ids,\n",
        "        'ab_token_type_ids':ab_token_type_ids,\n",
        "        'ab_attention_mask':ab_attention_mask,\n",
        "        'ba_input_ids':ba_input_ids,\n",
        "        'ba_token_type_ids':ba_token_type_ids,\n",
        "        'ba_attention_mask':ba_attention_mask,\n",
        "        'labels':labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "rUAzhxjSGl8G"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "06_self_Text_Data_Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}