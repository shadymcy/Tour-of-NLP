{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 3\n",
    "K = 15\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8/text8.train.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens:list[str] = tokenizer(text)\n",
    "\n",
    "counter = Counter(tokens).most_common(MAX_VOCAB_SIZE - 1)\n",
    "counter_sorted_by_freq = sorted(counter, key=lambda x: x[1], reverse=True)\n",
    "counter_dict = OrderedDict(counter_sorted_by_freq)\n",
    "\n",
    "vocabulary = vocab(counter_dict, specials=[\"<unk>\"],special_first = False)\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "word_counts = torch.tensor([count for count in counter_dict.values()], dtype=torch.float32)\n",
    "word_freqs = word_counts / torch.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vocabulary([\"apple\", \"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(data.Dataset):\n",
    "    def __init__(self, text: list[str], vocab: Vocab, word_freqs: torch.Tensor):\n",
    "        super(WordEmbeddingDataset, self).__init__()\n",
    "\n",
    "        self.text_encoded = torch.tensor(vocab(text), dtype=torch.long)\n",
    "        self.vocab = vocab\n",
    "        self.word_freqs = word_freqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        center_words = self.text_encoded[idx]\n",
    "\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "\n",
    "        pos_words_num = len(pos_indices)\n",
    "\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "\n",
    "        neg_words = torch.multinomial(\n",
    "            self.word_freqs, K * pos_words_num, replacement=True\n",
    "        )\n",
    "        \n",
    "        while len(set(pos_indices) & set(neg_words.tolist())) > 0:\n",
    "            neg_words = torch.multinomial(\n",
    "                self.word_freqs, K * pos_words_num, replacement=True\n",
    "            )\n",
    "\n",
    "        return center_words, pos_words, neg_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(tokens, vocabulary, word_freqs)\n",
    "dataloader = data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4997),\n",
       " tensor([  18,   10, 9999, 3098,   11,    5]),\n",
       " tensor([   9, 7386,  177,  390, 9202, 6696, 3054,    8,    0, 1552,   34, 5140,\n",
       "          281,  263, 3342,   50, 9694, 7388,  456,  350, 5468,    4, 6672, 3118,\n",
       "           24, 2078,   16, 3906,   29, 2005,   71,  395,  224, 1062,   87, 6866,\n",
       "         2570, 4883,  114, 3268, 3807,    0, 1033,  276, 1919,  532,  444, 1385,\n",
       "         2344, 3691, 4442, 1154,  197,    5, 4330,  130, 7619, 1889, 2640, 9707,\n",
       "         4630,  188,   29,  262, 3285,  137, 9952,  270, 2821,  479, 2818, 8008,\n",
       "          917, 2605, 1747, 6108,  475, 9238, 4524, 9572, 6651,   14,   51, 7351,\n",
       "         1489,   11,  812, 3627,   30, 6306]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VectorModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int):\n",
    "        super(Word2VectorModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.in_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, input_labels: torch.Tensor, pos_labels: torch.Tensor, neg_labels: torch.Tensor):\n",
    "        # [batch_size, embed_size, 1] 中心词只有一个\n",
    "        in_emb: torch.Tensor = self.in_embedding(input_labels).unsqueeze(2)\n",
    "        \n",
    "        # [batch_size, (C * 2), embedding_size] 相邻窗口词有C * 2个\n",
    "        pos_emb = self.out_embedding(pos_labels)\n",
    "        \n",
    "        # [batch_size, (C * 2 * K), embedding_size] 负采样词有C * 2 * K个\n",
    "        neg_emb = self.out_embedding(neg_labels)\n",
    "\n",
    "        pos_dot = torch.bmm(pos_emb, in_emb).squeeze(2)\n",
    "\n",
    "        neg_dot = torch.bmm(neg_emb, in_emb).squeeze(2)\n",
    "\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "\n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def input_embedding(self):\n",
    "        return self.in_embedding.weight.detach()\n",
    "\n",
    "model = Word2VectorModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = trange(EPOCHS)\n",
    "for e in epoch_bar:\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(tqdm(dataloader)):\n",
    "        input_labels = input_labels.to(DEVICE)\n",
    "        pos_labels = pos_labels.to(DEVICE)\n",
    "        neg_labels = neg_labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            epoch_bar.set_postfix(epoch=e + 1, iteration=i + 1, loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.input_embedding()\n",
    "torch.save(model.state_dict(), f\"embedding-{EMBEDDING_SIZE}.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6524]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_nearest(word):\n",
    "#     index = vocabulary([word])\n",
    "#     embedding = embedding_weights[index]\n",
    "#     cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "#     return [idx2word[i] for i in cos_dis.argsort()[:10]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0fd8daf3de7f45a9b49295aa3f29656f6d8734352d3946133bb02156a84307c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
