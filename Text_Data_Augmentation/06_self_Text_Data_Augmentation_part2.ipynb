{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_self_Text_Data_Augmentation_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bDd1SLjpFhY",
        "outputId": "8b26f723-9fde-4899-ad0c-1e9ab5665c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 18 13:57:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "#设置路径\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WrbHfHxpjWG",
        "outputId": "2828bbe0-ae2b-40e9-9096-72142f7021fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U synonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IitZuf_Cplw1",
        "outputId": "8b961e85-2e36-4b46-b29c-8e8fc2d89b93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting synonyms\n",
            "  Downloading synonyms-3.16.0.tar.gz (10.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from synonyms) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->synonyms) (1.1.0)\n",
            "Building wheels for collected packages: synonyms\n",
            "  Building wheel for synonyms (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for synonyms: filename=synonyms-3.16.0-py3-none-any.whl size=10832785 sha256=aa38ed22638fe148dd2920e0fe77ef7eaf1da750509e02ed3c832df59c4aa26e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cd/43/b4548753509a94471fc946967a07116252d49aeeb689db8f7c\n",
            "Successfully built synonyms\n",
            "Installing collected packages: synonyms\n",
            "Successfully installed synonyms-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frmB8caapm32",
        "outputId": "bb6f0f27-7cae-44f5-e096-2305d8bc9008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SSwE6yppv-",
        "outputId": "1706ec93-d3c3-4332-8a2a-974e2e66cdde"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.0.1\n",
            "  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.0.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.0.1) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.0.1) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.0.1) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=cbbb919e79a896002c40e7919f3c35de5bf1347d324ae02b0386bbda4922b88e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import synonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSoEhm8xprxX",
        "outputId": "f350e893-6c5c-470d-8955-e3ddd4e0960a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jieba] default dict file path ../data/vocab.txt\n",
            "[jieba] default dict file path ../data/vocab.txt\n",
            "[jieba] load default dict ../data/vocab.txt ...\n",
            "[jieba] load default dict ../data/vocab.txt ...\n",
            ">> Synonyms load wordseg dict [/usr/local/lib/python3.7/dist-packages/synonyms/data/vocab.txt] ... \n",
            ">> Synonyms on loading stopwords [/usr/local/lib/python3.7/dist-packages/synonyms/data/stopwords.txt] ...\n",
            "[Synonyms] on loading vectors [/usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz] ...\n",
            "\n",
            "[Synonyms] downloading data from https://github.com/chatopera/Synonyms/releases/download/3.15.0/words.vector.gz to /usr/local/lib/python3.7/dist-packages/synonyms/data/words.vector.gz ... \n",
            " this only happens if SYNONYMS_WORD2VEC_BIN_URL_ZH_CN is not present and Synonyms initialization for the first time. \n",
            " It would take minutes that depends on network.\n",
            "\n",
            "[Synonyms] downloaded.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA（Easy Data Augmentation）\n",
        "![EDA3](https://img-blog.csdnimg.cn/50c22b4212714b509ce053ff921d6bdd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "RIeS3F2QrMZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 对于训练集中的给定句子，随机选择并执行以下操作之一：\n",
        "* 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
        "* 随机插入 (RI)：在句子中随机找到一个词，并找出其同义词，且该同义词不是停用词。 将该同义词插入句子中的随机位置。 这样做n次。\n",
        "* 随机交换（RS）：随机选择句子中的两个单词并交换它们的位置。 这样做n次。\n",
        "* 随机删除（RD）：以概率 p 随机删除句子中的每个单词。"
      ],
      "metadata": {
        "id": "wuNtIMkDrSoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取停用词表\n",
        "import random\n",
        "import re\n",
        "from random import shuffle\n",
        "# strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
        "stop_words = {word.strip() for word in open('/content/drive/MyDrive/Colab Notebooks/dataset/baidu_stopwords.txt', 'r', encoding='utf8').readlines()}"
      ],
      "metadata": {
        "id": "9BUv5AFRptie"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 同义词替换（SR）：从句子中随机选择 n 个不是停用词的词。 用随机选择的同义词之一替换这些单词中的每一个。\n",
        "\n",
        "# 得到同义词列表\n",
        "def get_synonym(word):\n",
        "  # nearby返回一个元组，位置0返回同义词，位置1返回相似度\n",
        "  sys = set(synonyms.nearby(word)[0])\n",
        "  # 去除原词\n",
        "  if word in sys:\n",
        "    sys.remove(word)\n",
        "  return list(sys)\n",
        "  # 如果输入\"给力\" 可能没有同义词（同义词只有他自己） 则返回  ([],[])\n",
        "\n",
        "# 同义词替换\n",
        "# 传入一个词列表和替换词的数量\n",
        "def synonym_replacement(words, n):\n",
        "\n",
        "  new_words = words.copy()\n",
        "  # 去除停用词，去重，变成列表\n",
        "  random_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "  # 打乱\n",
        "  random.shuffle(random_word_list)\n",
        "\n",
        "  num_replaced = 0\n",
        "  for random_word in random_word_list:\n",
        "    synonym_words = get_synonym(random_word)\n",
        "    if len(synonym_words)>=1:\n",
        "        # 随机选取一个同义词\n",
        "        synonym = random.choice(list(synonym_words))\n",
        "        # 如果word是要替换的词 替换成同义词 ，否则返回原词\n",
        "        new_words = [synonym if word == random_word else word for word in new_words]\n",
        "        # 同义词替换数量+1\n",
        "        num_replaced += 1\n",
        "        \n",
        "    if num_replaced >= n:\n",
        "        break\n",
        "  # 为什么加这两句话？ 看note1\n",
        "  sentence = ' '.join(new_words)\n",
        "  new_words = sentence.split(' ')\n",
        "\n",
        "  return new_words"
      ],
      "metadata": {
        "id": "FoteXbv0swx1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用eda进行数据增强\n",
        "# 同义词替换的比例、 增强的句子数目\n",
        "def eda(sentence, alpha_sr=0.1, num_aug=9):\n",
        "\n",
        "  # 位置0是词组、1是词性\n",
        "  words = synonyms.seg(sentence)[0]\n",
        "  num_words = len(words)\n",
        "  n_sr = max(1, int(alpha_sr * num_words))\n",
        "\n",
        "  augmented_sentences = []\n",
        "\n",
        "  for _ in range(num_aug):\n",
        "    a_words = synonym_replacement(words, n_sr)\n",
        "    augmented_sentences.append(' '.join(a_words))\n",
        "  return augmented_sentences"
      ],
      "metadata": {
        "id": "HdAYQ0i5u69t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note1\n",
        "# 存在这样一种情况\n",
        "sentence = ['in', 'war', 'one', 'must', 'be', 'a', 'good', 'actor']\n",
        "word = 'actor'\n",
        "example_synonyms = ['actress', 'film star', 'performer', 'comedian', 'entertainer']\n",
        "new_sentence = ['in', 'war', 'one', 'must', 'be', 'a', 'good', 'film star']\n",
        "# 为了消除空格\n",
        "new_sentence = ['in', 'war', 'one', 'must', 'be', 'a', 'good', 'film', 'star']"
      ],
      "metadata": {
        "id": "5HHl5NL13t5z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda('9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz-l0Yxj30JT",
        "outputId": "90e10117-83b2-4edf-8dd8-d13b8a86f87e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['9 月底 15 日 以来 ， Canillac 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 英国 的 各类 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 英国 专利技术 ， 而 无法 发货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 手机 业务 将 遭受 受创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 分销商 ， 只要 没有 美国 的 相关 许可证 ， 全都 无法 供应 芯片 给 华为 ， 而 Nashik 等 换代 芯片 上市公司 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品 出现 货 不多 的 现象 ， 若 该 形势 急剧 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 LG 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关人员 许可证 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 手机 产品设计 出现 二手 多一些 的 现象 ， 若 该 形势 持续性 下去 ， 华为 手机 银行业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 新西兰 的 相关 许可证 ， 都 无法 供应 器件 给 华为 ， 而 SE9 等 国产 器件 企业 ， 也 因 采用 新西兰 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 SR 的 手机 产品 出现 货 少 的 现象 ， 若 该 情势 持续增长 下去 ， 华为 手机 相关服务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， GT5316SB0 、 高通 、 三星 等 华为 的 重要 竞争者 ， 只要 没有 哥伦比亚 的 相关 许可 ， 都 无法 配给 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 哥伦比亚 技术 ， 而 无法 交货 给 华为 。 目前 华为 部分 型号 的 电脑 产品 出现 货 少 的 现象 ， 若 该 形势 持续 下去 ， 华为 电脑 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可证 ， 都 无法 物资供应 芯片 给 华为 ， 而 中芯国际 等 国产化 芯片 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 改型 的 手机 产品 出现 货 少 的 乱象 ， 若 该 形势 在短期内 下去 ， 华为 手机 销售业务 将 遭致 重创 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 许可 ， 都 无法 供应 中央处理器 给 华为 ， 而 中芯国际 等 国产 中央处理器 企业 ， 也 因 采用 美国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 车种 的 手机 产品 出现 货 少 的 现象 ， 若 该 形势 继续 下去 ， 华为 手机 产品销售 将 遭致 摧残 。',\n",
              " '9 月 15 日 以来 ， 台积电 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 美国 的 相关 证 ， 都 无法 货源 芯片 给 华为 ， 而 中芯国际 等 国产化 芯片 外贸企业 ， 也 因 采用 美国 电子技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 改进型 的 手机 产品 出现 货 少 的 现象 ， 若 该 严峻形势 持续 下去 ， 华为 手机 业务 将 遭受 重创 。',\n",
              " '9 月 15 日 以来 ， 宏碁 、 高通 、 三星 等 华为 的 重要 合作伙伴 ， 只要 没有 英国 的 相关 许可权 ， 都 无法 供应 芯片 给 华为 ， 而 中芯国际 等 国产 芯片 企业 ， 也 因 采用 英国 技术 ， 而 无法 供货 给 华为 。 目前 华为 部分 型号 的 笔记本电脑 产品 出现 货 少 的 现象 ， 若 该 形势 持续性 下去 ， 华为 笔记本电脑 投资业务 将 遭致 重创 。']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![UDA5](https://img-blog.csdnimg.cn/9d10da70d1d0467e93ef5bb1267ac87f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)\n",
        "**$ \\tilde{\\theta} $： 参数不变 不进行反向传播**\n",
        "\n",
        "\n",
        "![在这里插入图片描述](https://img-blog.csdnimg.cn/88a3abe95bbd4e369fe4d085533c9c35.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTI4NzA2MA==,size_16,color_FFFFFF,t_70)"
      ],
      "metadata": {
        "id": "vPU8nQ-9Ap3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bucket_sampler import SortedSampler, BucketBatchSampler\n",
        "from EMA import *"
      ],
      "metadata": {
        "id": "JPf1XYme_WiE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "config = {\n",
        "        'train_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/train.json',\n",
        "        'dev_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/dev.json',\n",
        "        'test_file_path': '/content/drive/MyDrive/Colab Notebooks/dataset/ESIM/test.json',\n",
        "        'output_path': '.',\n",
        "        'model_path': '/content/drive/MyDrive/Colab Notebooks/dataset/BERT_model',\n",
        "        'batch_size': 16,\n",
        "        'num_epochs': 1,\n",
        "        'max_seq_len': 64,\n",
        "        'learning_rate': 2e-5,\n",
        "        'weight_decay': 0.01,\n",
        "        'use_bucket': True,\n",
        "        'bucket_multiplier': 200,\n",
        "        'unsup_data_ratio': 1.5,\n",
        "        'uda_softmax_temp': 0.4,\n",
        "        'uda_confidence_threshold': 0.8,\n",
        "        'device': 'cuda',\n",
        "        'n_gpus': 0,\n",
        "        'logging_step': 300,\n",
        "        'ema_start_step': 500,\n",
        "        'ema_start': False,\n",
        "        'seed': 2022\n",
        "    }\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    config['device'] = 'cpu'\n",
        "else:\n",
        "    config['n_gpus'] = torch.cuda.device_count()\n",
        "    config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "seed_everything(config['seed'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSIh_qnCFxnq",
        "outputId": "fb8829af-ccf7-4f0a-8fbc-423c7d24424f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2022"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
      ],
      "metadata": {
        "id": "Ed8eczWuH7sj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True, return_token_type_ids =True, return_attention_mask = True)\n",
        "\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)"
      ],
      "metadata": {
        "id": "aLobuIWSH9Za"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 对偶数据增强\n",
        "\n",
        "**a-b对，变成b-a对, 把两个句子换顺序**\\\n",
        "**我们的无监督数据增强就是用的对偶数据增强**\\\n",
        "**BERT输入 a，b两个句子，现在输入以b,a作为输入，增强样本**"
      ],
      "metadata": {
        "id": "p1OJnYZ3IQsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type != 'test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns = ['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "q2a4-zAUKFnS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 无监督BERT输入\n",
        "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  # 左右\n",
        "  lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens = True, return_token_type_ids = True, return_attention_mask = True)\n",
        "  # 右左\n",
        "  rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens = True, return_token_type_ids = True, return_attention_mask = True)\n",
        "\n",
        "  # 元组的形式\n",
        "  inputs['input_ids'].append((lr_inputs_dict['input_ids'], rl_inputs_dict['input_ids']))\n",
        "  inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'], rl_inputs_dict['token_type_ids']))\n",
        "  inputs['attention_mask'].append((lr_inputs_dict['attention_mask'], rl_inputs_dict['attention_mask']))\n",
        "  inputs['labels'].append(label)"
      ],
      "metadata": {
        "id": "ivfX_45aM8fb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## startswith()方法\n",
        "startswith() 方法用于检查字符串是否是以指定子字符串开头\\\n",
        "如果是则返回 True，否则返回 False。如果参数 beg 和 end 指定值，则在指定范围内检查。\\\n",
        "str.startswith(str, beg=0,end=len(string));\\\n",
        "*参数*\n",
        "```\n",
        "str --检测的字符串。\n",
        "strbeg --可选参数用于设置字符串检测的起始位置。\n",
        "strend --可选参数用于设置字符串检测的结束位置。\n",
        "```"
      ],
      "metadata": {
        "id": "0mKVUaZvs6GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type = 'train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type = 'dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type = 'test')\n",
        "\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "  processed_data = {}\n",
        "  unsup_data = defaultdict(list)\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "    if data_type == 'train':\n",
        "      reversed_inputs = defaultdict(list)\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total = len(df)):\n",
        "      label = 0 if data_type == 'test' else row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "      if data_type.startswith('test'):\n",
        "        build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
        "\n",
        "\n",
        "      build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "  processed_data['unsup_data'] = unsup_data\n",
        "  return processed_data\n",
        "\n",
        "# processed_data\n",
        "# {\n",
        "#    'train':,\n",
        "#    'dev':,\n",
        "#    'test':,\n",
        "#    'unsup_data':   # 数据量最大的\n",
        "# }"
      ],
      "metadata": {
        "id": "dwfnsojmrjBm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data(config, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxzjWiL3thie",
        "outputId": "82f66540-27d4-4a43-aa7f-213589ea0845"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 62042.19it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 103414.56it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 33412.10it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [01:04<00:00, 533.10it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:08<00:00, 537.76it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:09<00:00, 411.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "         self.data_dict['token_type_ids'][idx],\n",
        "         self.data_dict['attention_mask'][idx],\n",
        "         self.data_dict['labels'][idx])\n",
        "    return data\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])\n"
      ],
      "metadata": {
        "id": "dHu8cHCuVrH6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Collator:\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype = torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i, :seq_len] = torch.tensor(input_ids_list[i], dtype = torch.long)\n",
        "        token_type_ids[i, :seq_len] = torch.tensor(token_type_ids_list[i], dtype = torch.long)\n",
        "        attention_mask[i, :seq_len] = torch.tensor(attention_mask_list[i], dtype = torch.long)\n",
        "\n",
        "      else:\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len - 1] + [self.tokenizer.sep_token_id], dtype = torch.long)\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype = torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype = torch.long)\n",
        "\n",
        "    labels = torch.tensor(labels_list, dtype = torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "    data_dict = {\n",
        "        'input_ids':input_ids,\n",
        "        'token_type_ids':token_type_ids,\n",
        "        'attention_mask':attention_mask,\n",
        "        'labels':labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "jLRFtNPjWBV0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ],
      "metadata": {
        "id": "qU-ST7YHWnGT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UDA 无监督重新构造dataset 和 collator\n",
        "class UnsupAFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(UnsupAFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    input_ids = self.data_dict['input_ids'][idx]\n",
        "    token_type_ids = self.data_dict['token_type_ids'][idx]\n",
        "    attention_mask = self.data_dict['attention_mask'][idx]\n",
        "    labels = self.data_dict['labels'][idx]\n",
        "    # input_ids[0]：lr_inputs_dict['input_ids']    (build_unsup_bert_inputs)\n",
        "    # input_ids[1]：rl_inputs_dict['input_ids']\n",
        "    return (input_ids[0], token_type_ids[0], attention_mask[0],\n",
        "         input_ids[1], token_type_ids[1], attention_mask[1],\n",
        "         labels)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ],
      "metadata": {
        "id": "PzCTMQpXWp0q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnsupCollator(Collator):\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    super(UnsupCollator, self).__init__(max_seq_len, tokenizer)\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    # 根据UnsupAFQMCDataset的getitem 有七个数据\n",
        "    (ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list,\n",
        "     ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list,\n",
        "     labels_list) = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in ab_input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    # 分批整ab, ba（填充与截断）\n",
        "    ab_input_ids, ab_token_type_ids, ab_attention_mask, labels = self.pad_and_truncate(ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, labels_list, max_seq_len)\n",
        "    ba_input_ids, ba_token_type_ids, ba_attention_mask, labels = self.pad_and_truncate(ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "\n",
        "    data_dict = {\n",
        "        'ab_input_ids':ab_input_ids,\n",
        "        'ab_token_type_ids':ab_token_type_ids,\n",
        "        'ab_attention_mask':ab_attention_mask,\n",
        "        'ba_input_ids':ba_input_ids,\n",
        "        'ba_token_type_ids':ba_token_type_ids,\n",
        "        'ba_attention_mask':ba_attention_mask,\n",
        "        'labels':labels\n",
        "    }\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "THRD_vhSZhPS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "def build_dataloader(config, data, tokenizer):\n",
        "  train_dataset = AFQMCDataset(data['train'])\n",
        "  dev_dataset = AFQMCDataset(data['dev'])\n",
        "  test_dataset = AFQMCDataset(data['test'])\n",
        "  unsup_dataset = UnsupAFQMCDataset(data['unsup_data'])\n",
        "\n",
        "  collate_fn = Collator(config['max_seq_len'], tokenizer)\n",
        "  unsup_collate_fn = UnsupCollator(config['max_seq_len'], tokenizer)\n",
        "\n",
        "  # 使用桶采样\n",
        "  if config['use_bucket']:\n",
        "    # 监督数据\n",
        "    # 基采样器RandomSampler\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    # drop_last 最后一个batch小于size 丢弃\n",
        "    bucket_sampler = BucketBatchSampler(train_sampler, batch_size = config['batch_size'],\n",
        "                      drop_last = False, sort_key = lambda x: len(train_dataset[x][0]),# 以 input_id 长度作为排序的指标\n",
        "                      bucket_size_multiplier = config['bucket_multiplier'])\n",
        "    train_dataloader = DataLoader(dataset = train_dataset, batch_sampler = bucket_sampler, num_workers = 4, collate_fn = collate_fn)\n",
        "    # 无监督数据 \n",
        "    # grad_data中(图) 有监督、无监督一起训练， 无监督数据量大， batchsize可以设置大一点\n",
        "    unsup_sampler = RandomSampler(unsup_dataset)\n",
        "    unsup_bucket_sampler = BucketBatchSampler(unsup_sampler, batch_size = int(config['batch_size'] * config['unsup_data_ratio']),\n",
        "                      drop_last = False, sort_key = lambda x: len(unsup_dataset[x][0]),# 以 input_id 长度作为排序的指标\n",
        "                      bucket_size_multiplier = config['bucket_multiplier'])\n",
        "    \n",
        "    unsup_dataloader = DataLoader(dataset = unsup_dataset, batch_sampler = unsup_bucket_sampler, num_workers = 4, collate_fn = unsup_collate_fn)\n",
        "  # 不使用桶采样\n",
        "  else:\n",
        "    # 监督数据\n",
        "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = config['batch_size'], shuffle = True, num_workers = 4, collate_fn = collate_fn)\n",
        "    # 无监督数据\n",
        "    unsup_dataloader = DataLoader(dataset = unsup_dataset, batch_size = int(config['batch_size'] * config['unsup_data_ratio']), \n",
        "                    shuffle = True, num_workers = 4, collate_fn = unsup_collate_fn)\n",
        "  # 验证集、测试集dataloader 与桶采样无关 因为shuffle=false  \n",
        "  dev_dataloader = DataLoader(dataset = dev_dataset, batch_size = config['batch_size'], shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "  test_dataloader = DataLoader(dataset = test_dataset, batch_size = config['batch_size'], shuffle = False, num_workers = 4, collate_fn = collate_fn)\n",
        "\n",
        "  return unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "CFS6iR02a6Ba"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, data, tokenizer)"
      ],
      "metadata": {
        "id": "V7YWrqMYb516"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc = 'Evaluation', total = len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch['labels'])\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "      batch_cuda['mode'] = 'val'\n",
        "      loss, logits = model(**batch_cuda)[:2]\n",
        "\n",
        "      if config['n_gpus'] > 1:\n",
        "        loss = loss.mean()\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      preds.append(logits.argmax(dim = -1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss / len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim = 0).numpy()\n",
        "  preds = torch.cat(preds, dim = 0).numpy()\n",
        "  f1 = f1_score(labels, preds)\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return avg_val_loss, f1, acc"
      ],
      "metadata": {
        "id": "ljaJQ6mLcIoJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "class BertForAFQMC(BertForSequenceClassification):\n",
        "   # 复写forward\n",
        "   def forward(self, input_ids, token_type_ids, attention_mask, labels = None, mode= 'train'):\n",
        "\n",
        "     outputs = self.bert(input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask, output_hidden_states = True)\n",
        "     # 维度：[batch_size, hidden_size]\n",
        "     pooled_output = outputs[1]\n",
        "     pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "     logits = self.classifier(pooled_output)\n",
        "     # print('BertForAFQMC中logits：',logits)\n",
        "     outputs = (logits, )\n",
        "     # print('BertForAFQMC中outputs：',outputs)\n",
        "\n",
        "     if mode == 'val':\n",
        "       loss_fct = nn.CrossEntropyLoss()\n",
        "       # X.view(-1)中的-1本意是根据另外一个数来自动调整维度\n",
        "       loss = loss_fct(logits, labels.view(-1))\n",
        "\n",
        "       outputs = (loss, ) + outputs\n",
        "     return outputs"
      ],
      "metadata": {
        "id": "CEdDR6ZUcbJ5"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}