## us-patent

*参考论文:*
1.DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\
https://arxiv.org/abs/2111.09543

2.DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\
https://arxiv.org/abs/1910.01108

3.ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations\
https://arxiv.org/abs/1909.11942
